

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd


[OSEv3:vars]
ansible_user=root

#openshift_enable_excluders=False
openshift_use_crio=False
openshift_use_crio_only=False
containerized=False
deployment_type=origin
openshift_deployment_type=origin
#openshift_portal_net=172.30.0.0/16
#osm_cluster_network_cidr=10.128.0.0/14
#osm_host_subnet_length=9
#openshift_node_groups=[{'name': 'node-config-all-in-one', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}]

#yum install centos-release-openshift-origin311
# OpenShift repository configuration
openshift_additional_repos=[{'id': 'centos-paas', 'name': 'centos-paas', 'baseurl' :'https://cbs.centos.org/repos/paas7-openshift-origin311-release/x86_64/os/', 'gpgcheck' :'0', 'enabled' :'1'}]
#openshift_additional_repos=[{'id': 'centos-paas', 'name': 'centos-paas', 'baseurl' :'https://buildlogs.centos.org/centos/7/paas/x86_64/openshift-origin311', 'gpgcheck' :'0', 'enabled' :'1'}]
#openshift_repos_enable_testing=false

# Specify an exact rpm version to install or configure.
# WARNING: This value will be used for all hosts in RPM based environments, even those that have another version installed.
# This could potentially trigger an upgrade and downtime, so be careful with modifying this value after the cluster is set up.
#openshift_pkg_version=-3.11.0
openshift_release="3.11"
openshift_version="3.11"

openshift_master_default_subdomain=sol.oondeo.es

#Set cluster_hostname to point at your load balancer
#openshift_master_cluster_hostname=console.sol.oondeo.es
openshift_master_cluster_public_hostname=console.sol.oondeo.es

openshift_disable_check=disk_availability,docker_storage,memory_availability,docker_image_availability
openshift_node_kubelet_args={'pods-per-core': ['10']}


# Debug level for all OpenShift components (Defaults to 2)
debug_level=0

# Manage openshift example imagestreams and templates during install and upgrade
openshift_install_examples=false

# Configure imagePolicyConfig in the master config
# See: https://docs.okd.io/latest/admin_guide/image_policy.html
#openshift_master_image_policy_config={"maxImagesBulkImportedPerRepository": 3, "disableScheduledImport": true}
# This setting overrides allowedRegistriesForImport in openshift_master_image_policy_config. By default, all registries are allowed.
#openshift_master_image_policy_allowed_registries_for_import=["docker.io", "*.docker.io", "*.redhat.com", "gcr.io", "quay.io", "registry.centos.org", "registry.redhat.io", "*.amazonaws.com"]

# Configure master API rate limits for external clients
#openshift_master_external_ratelimit_qps=200
#openshift_master_external_ratelimit_burst=400
# Configure master API rate limits for loopback clients
#openshift_master_loopback_ratelimit_qps=300
#openshift_master_loopback_ratelimit_burst=600


# Items added, as is, to end of /etc/sysconfig/docker OPTIONS
# Default value: "--log-driver=journald"
#openshift_docker_options="-l warn --ipv6=false"

# Specify exact version of Docker to configure or upgrade to.
# Downgrades are not supported and will error out. Be careful when upgrading docker from < 1.10 to > 1.10.
# docker_version="1.12.1"

# Enable etcd debug logging, defaults to false
# etcd_debug=true
# Set etcd log levels by package
# etcd_log_package_levels="etcdserver=WARNING,security=DEBUG"
# Comma-separated list of etcd cipher suites
# etcd_cipher_suites="TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256"

# Upgrade Hooks
#
# Hooks are available to run custom tasks at various points during a cluster
# upgrade. Each hook should point to a file with Ansible tasks defined. Suggest using
# absolute paths, if not the path will be treated as relative to the file where the
# hook is actually used.
#
# Tasks to run before each master is upgraded.
# openshift_master_upgrade_pre_hook=/usr/share/custom/pre_master.yml
#
# Tasks to run to upgrade the master. These tasks run after the main openshift-ansible
# upgrade steps, but before we restart system/services.
# openshift_master_upgrade_hook=/usr/share/custom/master.yml
#
# Tasks to run after each master is upgraded and system/services have been restarted.
# openshift_master_upgrade_post_hook=/usr/share/custom/post_master.yml

# Cluster Image Source (registry) configuration
# openshift-enterprise default is 'registry.redhat.io/openshift3/ose-${component}:${version}'
# origin default is 'docker.io/openshift/origin-${component}:${version}'
#oreg_url=example.com/openshift3/ose-${component}:${version}
# If oreg_url points to a registry other than registry.redhat.io we can
# modify image streams to point at that registry by setting the following to true
#openshift_examples_modify_imagestreams=true
# Add insecure and blocked registries to global docker configuration
#openshift_docker_insecure_registries=registry.example.com
#openshift_docker_blocked_registries=registry.hacker.com
# You may also configure additional default registries for docker, however this
# is discouraged. Instead you should make use of fully qualified image names.
#openshift_docker_additional_registries=registry.example.com

# If oreg_url points to a registry requiring authentication, provide the following:
#oreg_auth_user=some_user
#oreg_auth_password='my-pass'
# NOTE: oreg_url must be defined by the user for oreg_auth_* to have any affect.
# oreg_auth_pass should be generated from running docker login.

openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]
openshift_master_htpasswd_file='{{ inventory_dir }}/oondeo~/htpasswd'

# LDAP auth
#openshift_master_identity_providers=[{'name': 'my_ldap_provider', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['dn'], 'email': ['mail'], 'name': ['cn'], 'preferredUsername': ['uid']}, 'bindDN': '', 'bindPassword': '', 'insecure': 'false', 'url': 'ldap://ldap.example.com:389/ou=users,dc=example,dc=com?uid'}]
#
# Configure LDAP CA certificate
# Specify either the ASCII contents of the certificate or the path to
# the local file that will be copied to the remote host. CA
# certificate contents will be copied to master systems and saved
# within /etc/origin/master/ with a filename matching the "ca" key set
# within the LDAPPasswordIdentityProvider.
#
#openshift_master_ldap_ca=<ca text>
# or
#openshift_master_ldap_ca_file=<path to local ca file to use>

# OpenID auth
#openshift_master_identity_providers=[{"name": "openid_auth", "login": "true", "challenge": "false", "kind": "OpenIDIdentityProvider", "client_id": "openshift", "client_secret": "d&vxQKt#[72~v]9V<$5nx5C&", "claims": {"id": ["sub"], "preferredUsername": ["preferred_username"], "name": ["name"], "email": ["email"]}, "urls": {"authorize": "https://auth.oondeo.es/oauth2/authorize", "token": "https://auth.oondeo.es/oauth2/token"}]
#
# Configure OpenID CA certificate
# Specify either the ASCII contents of the certificate or the path to
# the local file that will be copied to the remote host. CA
# certificate contents will be copied to master systems and saved
# within /etc/origin/master/ with a filename matching the "ca" key set
# within the OpenIDIdentityProvider.
#
#openshift_master_openid_ca=<ca text>
# or
#openshift_master_openid_ca_file="{{ inventory_dir }}/oondeo~/ssl/ca.cer"


# Project Configuration
#osm_project_request_message=''
#osm_project_request_template=''
#osm_mcs_allocator_range='s0:/2'
#osm_mcs_labels_per_project=5
#osm_uid_allocator_range='1000000000-1999999999/10000'

# Configure additional projects
#openshift_additional_projects={'my-project': {'default_node_selector': 'label=value'}}

# Enable cockpit
osm_use_cockpit=false
#
# Set cockpit plugins
#osm_cockpit_plugins=['cockpit-kubernetes']

# Configure controller arguments
#osm_controller_args={'resource-quota-sync-period': ['10s']}

# Configure api server arguments
#osm_api_server_args={'max-requests-inflight': ['400']}

# additional cors origins
#osm_custom_cors_origins=['foo.example.com', 'bar.example.com']

# default project node selector
#osm_default_node_selector='region=primary'

# Override the default pod eviction timeout
#openshift_master_pod_eviction_timeout=5m

# Override the default oauth tokenConfig settings:
# openshift_master_access_token_max_seconds=86400
# openshift_master_auth_token_max_seconds=500

# Override master servingInfo.maxRequestsInFlight
#openshift_master_max_requests_inflight=500

# OpenShift Router Options
#
# An OpenShift router will be created during install if there are
# nodes present with labels matching the default router selector,
# "node-role.kubernetes.io/infra=true".
#
# Example:
# [nodes]
# node.example.com openshift_node_group_name="node-config-infra"
#
# Router selector (optional)
# Router will only be created if nodes matching this label are present.
# Default value: 'node-role.kubernetes.io/infra=true'
#openshift_hosted_router_selector='node-role.kubernetes.io/infra=true'
#
# Router replicas (optional)
# Unless specified, openshift-ansible will calculate the replica count
# based on the number of nodes matching the openshift router selector.
#openshift_hosted_router_replicas=2
#
# Router extended route validation (optional)
# If enabled, openshift-ansible will configure the router to perform extended
# validation on routes before admitting them.
#openshift_hosted_router_extended_validation=true
#
# Router force subdomain (optional)
# A router path format to force on all routes used by this router
# (will ignore the route host value)
openshift_hosted_router_force_subdomain='${name}-${namespace}.sol.oondeo.es'
#
# Router certificate (optional)
# Provide local certificate paths which will be configured as the
# router's default certificate.
openshift_hosted_router_certificate={"certfile": "{{ inventory_dir }}/oondeo~/ssl/sylar.oondeo.es.cer", "keyfile": "{{ inventory_dir }}/oondeo~/ssl/sylar.oondeo.es.key", "cafile": "{{ inventory_dir }}/oondeo~/ssl/ca.cer"}
#
# Manage the OpenShift Router (optional)
#openshift_hosted_manage_router=true
#
# Router sharding support has been added and can be achieved by supplying the correct
# data to the inventory.  The variable to house the data is openshift_hosted_routers
# and is in the form of a list.  If no data is passed then a default router will be
# created.  There are multiple combinations of router sharding.  The one described
# below supports routers on separate nodes.
#
#openshift_hosted_routers=[{'name': 'router1', 'certificate': {'certfile': '/path/to/certificate/abc.crt', 'keyfile': '/path/to/certificate/abc.key', 'cafile': '/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router', 'namespace': 'default', 'stats_port': 1936, 'edits': [], 'images': 'openshift3/ose-${component}:${version}', 'selector': 'type=router1', 'ports': ['80:80', '443:443']}, {'name': 'router2', 'certificate': {'certfile': '/path/to/certificate/xyz.crt', 'keyfile': '/path/to/certificate/xyz.key', 'cafile': '/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router', 'namespace': 'default', 'stats_port': 1936, 'edits': [{'action': 'append', 'key': 'spec.template.spec.containers[0].env', 'value': {'name': 'ROUTE_LABELS', 'value': 'route=external'}}], 'images': 'openshift3/ose-${component}:${version}', 'selector': 'type=router2', 'ports': ['80:80', '443:443']}]

# OpenShift Registry Console Options
# Override the console image prefix:
# origin default is "cockpit/", enterprise default is "openshift3/"
#openshift_cockpit_deployer_prefix=registry.example.com/myrepo/
# origin default is "kubernetes", enterprise default is "registry-console"
#openshift_cockpit_deployer_basename=my-console
# Override image version, defaults to latest for origin, vX.Y product version for enterprise
#openshift_cockpit_deployer_version=1.4.1

# Registry selector (optional)
# Registry will only be created if nodes matching this label are present.
# Default value: 'node-role.kubernetes.io/infra=true'
#openshift_hosted_registry_selector='node-role.kubernetes.io/infra=true'
#
# Registry replicas (optional)
# Unless specified, openshift-ansible will calculate the replica count
# based on the number of nodes matching the openshift registry selector.
#openshift_hosted_registry_replicas=2
#
# Validity of the auto-generated certificate in days (optional)
#openshift_hosted_registry_cert_expire_days=730
#
# Manage the OpenShift Registry (optional)
#openshift_hosted_manage_registry=true
# Manage the OpenShift Registry Console (optional)
#openshift_hosted_manage_registry_console=true
#
# hostPath (local filesystem storage)
# Suitable for "all-in-one" or proof of concept deployments
# Must not be used for high-availability and production deployments
#openshift_hosted_registry_storage_kind=hostpath
#openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
#openshift_hosted_registry_storage_hostpath_path=/var/lib/origin/openshift_volumes
#openshift_hosted_registry_storage_volume_size=10Gi

# Metrics deployment
# See: https://docs.openshift.com/container-platform/latest/install_config/cluster_metrics.html
#
# By default metrics are not automatically deployed, set this to enable them
openshift_metrics_install_metrics=false
#
# metrics-server deployment
# By default, metrics-server is not automatically deployed, unless metrics is also
# deployed.  Deploying metrics-server is necessary to use the HorizontalPodAutoscaler.
# Set this to enable it.
openshift_metrics_server_install=false
#
# Storage Options
# If openshift_metrics_storage_kind is unset then metrics will be stored
# in an EmptyDir volume and will be deleted when the cassandra pod terminates.
# Storage options A & B currently support only one cassandra pod which is
# generally enough for up to 1000 pods. Additional volumes can be created
# manually after the fact and metrics scaled per the docs.
#


# PersistentLocalStorage
# If Persistent Local Storage is wanted, this boolean can be defined to True.
# This will create all necessary configuration to use persistent storage on nodes.
openshift_persistentlocalstorage_enabled=true
openshift_persistentlocalstorage_classes=['hdd']
openshift_persistentlocalstorage_path=/mnt/local-storage
openshift_persistentlocalstorage_provisionner_image=quay.io/external_storage/local-volume-provisioner:v1.0.1

# Cluster monitoring
#
# Cluster monitoring is enabled by default, disable it by setting
openshift_cluster_monitoring_operator_install=false
#
# Cluster monitoring configuration variables allow setting the amount of
# storage requested through PersistentVolumeClaims.
#
# openshift_cluster_monitoring_operator_prometheus_storage_capacity="50Gi"
# openshift_cluster_monitoring_operator_alertmanager_storage_capacity="2Gi"

# Grafana Configuration
#grafana_namespace=grafana
#grafana_user=grafana
#grafana_password=grafana
#grafana_datasource_name="example"
#grafana_prometheus_namespace="openshift-metrics"
#grafana_prometheus_sa=prometheus
#grafana_node_exporter=false
#grafana_graph_granularity="5m"

# Logging deployment
#
# Currently logging deployment is disabled by default, enable it by setting this
openshift_logging_install_logging=false
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra":"true"}
#openshift_logging_storage_kind=emptydir

#
# Option C - Dynamic -- If openshift supports dynamic volume provisioning for
# your cloud platform use this.
#openshift_logging_storage_kind=none
#
# Option D - none -- Logging will use emptydir volumes which are destroyed when
# pods are deleted
#
# Other Logging Options -- Common items you may wish to reconfigure, for the complete
# list of options please see roles/openshift_logging/README.md
#
# Configure loggingPublicURL in the master config for aggregate logging, defaults
# to kibana.{{ openshift_master_default_subdomain }}
openshift_logging_kibana_hostname=log.sol.oondeo.es
# Configure the number of elastic search nodes, unless you're using dynamic provisioning
# this value must be 1
openshift_logging_es_cluster_size=1

os_firewall_use_firewalld=false

#openshift_use_openshift_sdn=false
#openshift_use_contiv=true
#os_sdn_network_plugin_name='cni'

# Use the following contiv settings for now. 
# In upcoming versions of the installer, these settings will be automatically defaulted and hence not needed to be configured explicitly here
#contiv_netplugin_interface=eth1
#contiv_netmaster_interface=eth1
#contiv_netplugin_fwd_mode=routing
#contiv_encap_mode=vxlan
#contiv_default_network_tag=1000
#use_contiv_etcd=False

# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#
# WARNING : Do not pick subnets that overlap with the default Docker bridge subnet of
# 172.17.0.0/16.  Your installation will fail and/or your configuration change will
# cause the Pod SDN or Cluster SDN to fail.
#
# WORKAROUND : If you must use an overlapping subnet, you can configure a non conflicting
# docker0 CIDR range by adding '--bip=192.168.2.1/24' to DOCKER_NETWORK_OPTIONS
# environment variable located in /etc/sysconfig/docker-network.
# When upgrading or scaling up the following must match whats in your master config!
#  Inventory: master yaml field
#  osm_cluster_network_cidr: clusterNetworkCIDR
#  openshift_portal_net: serviceNetworkCIDR
# When installing osm_cluster_network_cidr and openshift_portal_net must be set.
# Sane examples are provided below.
#osm_cluster_network_cidr=10.128.0.0/14
#openshift_portal_net=172.30.0.0/16

# ExternalIPNetworkCIDRs controls what values are acceptable for the
# service external IP field. If empty, no externalIP may be set. It
# may contain a list of CIDRs which are checked for access. If a CIDR
# is prefixed with !, IPs in that CIDR will be rejected. Rejections
# will be applied first, then the IP checked against one of the
# allowed CIDRs. You should ensure this range does not overlap with
# your nodes, pods, or service CIDRs for security reasons.
#openshift_master_external_ip_network_cidrs=['0.0.0.0/0']

# IngressIPNetworkCIDR controls the range to assign ingress IPs from for
# services of type LoadBalancer on bare metal. If empty, ingress IPs will not
# be assigned. It may contain a single CIDR that will be allocated from. For
# security reasons, you should ensure that this range does not overlap with
# the CIDRs reserved for external IPs, nodes, pods, or services.
#openshift_master_ingress_ip_network_cidr=172.46.0.0/16

# Configure number of bits to allocate to each host's subnet e.g. 9
# would mean a /23 network on the host.
# When upgrading or scaling up the following must match whats in your master config!
#  Inventory: master yaml field
#  osm_host_subnet_length:  hostSubnetLength
# When installing osm_host_subnet_length must be set. A sane example is provided below.
#osm_host_subnet_length=9

# Configure master API and console ports.
#openshift_master_api_port=8443
#openshift_master_console_port=8443

# set exact RPM version (include - prefix)
#openshift_pkg_version=-3.11.0
# you may also specify version and release, ie:
#openshift_pkg_version=-3.11.0-0.126.0.git.0.9351aae.el7

# Configure custom ca certificate
#openshift_master_ca_certificate={'certfile': '/path/to/ca.crt', 'keyfile': '/path/to/ca.key'}
#
# NOTE: CA certificate will not be replaced with existing clusters.
# This option may only be specified when creating a new cluster or
# when redeploying cluster certificates with the redeploy-certificates
# playbook.

# Configure custom named certificates (SNI certificates)
#
# https://docs.okd.io/latest/install_config/certificate_customization.html
# https://docs.openshift.com/container-platform/latest/install_config/certificate_customization.html
#
# NOTE: openshift_master_named_certificates is cached on masters and is an
# additive fact, meaning that each run with a different set of certificates
# will add the newly provided certificates to the cached set of certificates.
#
# An optional CA may be specified for each named certificate. CAs will
# be added to the OpenShift CA bundle which allows for the named
# certificate to be served for internal cluster communication.
#
# If you would like openshift_master_named_certificates to be overwritten with
# the provided value, specify openshift_master_overwrite_named_certificates.
openshift_master_overwrite_named_certificates=true
openshift_master_named_certificates=[{"certfile": "{{ inventory_dir }}/oondeo~/ssl/sylar.oondeo.es.cer", "keyfile": "{{ inventory_dir }}/oondeo~/ssl/sylar.oondeo.es.key", "names": ["ovh"], "cafile": "{{ inventory_dir }}/oondeo~/ssl/ca.cer"}]#

# Configure logrotate scripts
# See: https://github.com/nickhammond/ansible-logrotate
#logrotate_scripts=[{"name": "syslog", "path": "/var/log/cron\n/var/log/maillog\n/var/log/messages\n/var/log/secure\n/var/log/spooler\n", "options": ["daily", "rotate 7", "compress", "sharedscripts", "missingok"], "scripts": {"postrotate": "/bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true"}}]

# Enable service catalog
openshift_enable_service_catalog=true

# Enable template service broker (requires service catalog to be enabled, above)
template_service_broker_install=true

# Specify an openshift_service_catalog image
# (defaults for origin and openshift-enterprise, repsectively)
#openshift_service_catalog_image="docker.io/openshift/origin-service-catalog:{{ openshift_image_tag }}""
#openshift_service_catalog_image="registry.redhat.io/openshift3/ose-service-catalog:{{ openshift_image_tag }}"

# Configure one of more namespaces whose templates will be served by the TSB
#openshift_template_service_broker_namespaces=['openshift']

# masterConfig.volumeConfig.dynamicProvisioningEnabled, configurable as of 1.2/3.2, enabled by default
#openshift_master_dynamic_provisioning_enabled=true

# Admission plugin config
#openshift_master_admission_plugin_config={"ProjectRequestLimit":{"configuration":{"apiVersion":"v1","kind":"ProjectRequestLimitConfig","limits":[{"selector":{"admin":"true"}},{"maxProjects":"1"}]}},"PodNodeConstraints":{"configuration":{"apiVersion":"v1","kind":"PodNodeConstraintsConfig"}}}

# Configure usage of openshift_clock role.
#openshift_clock_enabled=true

# OpenShift Per-Service Environment Variables
# Environment variables are added to /etc/sysconfig files for
# each OpenShift node.
# API and controllers environment variables are merged in single
# master environments.
#openshift_node_env_vars={"ENABLE_HTTP2": "true"}



# Firewall configuration
# You can open additional firewall ports by defining them as a list. of service
# names and ports/port ranges for either masters or nodes.
#openshift_master_open_ports=[{"service":"svc1","port":"11/tcp"}]
#openshift_node_open_ports=[{"service":"svc2","port":"12-13/tcp"},{"service":"svc3","port":"14/udp"}]

# Service port node range
#openshift_node_port_range=30000-32767

# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
openshift_enable_unsupported_configurations=true


[masters]
sylar.cluster.oondeo.es 

[etcd]
sylar.cluster.oondeo.es 

[nodes]
sylar.cluster.oondeo.es  openshift_hostname=sylar openshift_public_hostname=sylar.cluster.oondeo.es openshift_ip=172.18.0.2 openshift_node_group_name="node-config-all-in-one"  openshift_schedulable=true


